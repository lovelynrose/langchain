{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngl5ECsiG_YC",
        "outputId": "66b9a7c7-78c3-402b-bd1b-40eb3245d094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.350-py3-none-any.whl (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain)\n",
            "  Downloading langchain_community-0.0.3-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1 (from langchain)\n",
            "  Downloading langchain_core-0.1.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.71-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.350 langchain-community-0.0.3 langchain-core-0.1.1 langsmith-0.0.71 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "dUl-SXn7HM61"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PromptTemplate"
      ],
      "metadata": {
        "id": "6CDrnaIZKdAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Prompt to text completion models\n",
        "*   Input : String\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wy8kddRuPnhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(*,<br> input_variables: List[str], <br>\n",
        "input_types: Dict[str, Any] = dict,<br>\n",
        "output_parser: BaseOutputParser | None = None, <br>\n",
        "partial_variables: Mapping[str, str | (() -> str)] = dict, <br>\n",
        "template: str, <br>\n",
        "template_format: Literal['f-string', 'jinja2'] = \"f-string\", <br>\n",
        "validate_template: bool = False) -> None<br>\n"
      ],
      "metadata": {
        "id": "OI16mfZCKMtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiation using initializer"
      ],
      "metadata": {
        "id": "KnZCb1byMzwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = PromptTemplate(\n",
        "  input_variables=[\"topic\",\"person\"],\n",
        "  template= \"Define {topic} for a {person} person\",\n",
        "  template_format='f-string',\n",
        "  validate_template=True\n",
        ")"
      ],
      "metadata": {
        "id": "LHwPBvqlIeTt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(template,type(template))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUmLd1-sLRea",
        "outputId": "d5937453-9528-429f-9cdd-fe2ce0c9434d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['topic', 'person'] template='Define {topic} for a {person} person' validate_template=True <class 'langchain_core.prompts.prompt.PromptTemplate'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template.format(person = \"experienced\", topic=\"GAI\")\n",
        "print(prompt,type(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWzj37NeL0Gt",
        "outputId": "6b56d72f-6950-4e0e-83ab-c88234c90510"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define GAI for a experienced person <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiation using from_template (recommended)"
      ],
      "metadata": {
        "id": "wJjsFAv8M85n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def from_template(cls,<br> template: str,<br> *, template_format: str='f-string',<br> partial_variables: Optional[Dict[str, Any]]=None,<br> **kwargs: Any) -> PromptTemplate"
      ],
      "metadata": {
        "id": "Sa96iNhTOUJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = PromptTemplate.from_template(\n",
        "  template= \"Define {topic} for a {person} person\",\n",
        "  template_format='f-string', #default\n",
        "  validate_template=True\n",
        ")"
      ],
      "metadata": {
        "id": "Zr4fUgnwLYwf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(template,type(template))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1KuHxRTZo5X",
        "outputId": "ff516d99-951c-49c0-c7a0-52b705422d66"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['person', 'topic'] template='Define {topic} for a {person} person' validate_template=True <class 'langchain_core.prompts.prompt.PromptTemplate'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template.format(person = \"experienced\", topic=\"GAI\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv0sqd6VZsKs",
        "outputId": "f67cac9d-9203-4141-cb71-95a7d9be77c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define GAI for a experienced person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PromptTemplate with no arguments"
      ],
      "metadata": {
        "id": "lB_yzKYLCVYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate.from_template(\"What is GAI\")\n",
        "print(type(template),template)\n",
        "prompt = template.format()\n",
        "print(type(prompt),prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUl1bjVHCXww",
        "outputId": "928d2de2-6afd-40c8-d1a7-82f2b7f756f5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.prompts.prompt.PromptTemplate'> input_variables=[] template='What is GAI'\n",
            "<class 'str'> What is GAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PromptTemplate with 1 keyword argument"
      ],
      "metadata": {
        "id": "5Zb6y5I0DBgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate.from_template(\"What is {topic}\")\n",
        "prompt = template.format(topic = \"GAI\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er6QYGDvDE67",
        "outputId": "33c5239f-11c3-47a6-8793-0776b28b3a61"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is GAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PromptTemplate with more than 1 argument"
      ],
      "metadata": {
        "id": "ugFBS0c9Elx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate.from_template(template = \"Write about {topic} for {person}\",\n",
        "                                        template_format = \"f-string\",\n",
        "                                        )\n",
        "prompt = template.format(topic = \"GAI\", person = \"beginner\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8ZImIW_ElFV",
        "outputId": "065ebb9d-15fd-464f-846f-fd8953c75f5d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write about GAI for beginner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same template for varying Values"
      ],
      "metadata": {
        "id": "sho8uhwfI7wW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Define a simple prompt template\n",
        "prompt_template = PromptTemplate.from_template(template=\"What is {topic}?\")\n",
        "\n",
        "# Generate formatted prompts with different topics\n",
        "for topic in [\"GAI\", \"NLP\", \"ML\"]:\n",
        "    formatted_prompt = prompt_template.format(topic=topic)\n",
        "    print(formatted_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoCaXMjEIrzu",
        "outputId": "c8836f6c-0baf-4ce3-fa40-ea94c9d425cf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is GAI?\n",
            "What is NLP?\n",
            "What is ML?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validate Template"
      ],
      "metadata": {
        "id": "cLMxa31OQd0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate(\n",
        "              input_variables = [\"topic\",\"person\",\"extra\"],\n",
        "              template = \"Define {topic} for {person}\",\n",
        "              validate_template = False\n",
        "              )\n",
        "prompt = template.format(person = \"experienced\", topic=\"GAI\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YApS0NrUXjv",
        "outputId": "4b9393b6-e948-494c-df34-227ce7138805"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define GAI for experienced\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate(\n",
        "              input_variables = [\"topic\",\"person\",\"extra\"],\n",
        "              template = \"Define {topic} for {person}\",\n",
        "              validate_template = True\n",
        "              )\n",
        "prompt = template.format(person = \"experienced\", topic=\"GAI\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "3hPBuXqKU2GX",
        "outputId": "a38755c2-9308-4214-ffb0-45636a55f8a6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-1acbbe0423a0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m template = PromptTemplate(\n\u001b[0m\u001b[1;32m      4\u001b[0m               \u001b[0minput_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"topic\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"person\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"extra\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Define {topic} for {person}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for PromptTemplate\n__root__\n  Invalid prompt schema; check for mismatched or missing input parameters from ['topic', 'person', 'extra']. (type=value_error)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Pipelining"
      ],
      "metadata": {
        "id": "r9PSI6vfYUSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template1 = \"Define {topic}\"\n",
        "template2 = \" for {person} people\"\n",
        "template3 = \" Use a {tone} tone\"\n",
        "\n",
        "template = PromptTemplate(\n",
        "              template = template1 \\\n",
        "                + template2 \\\n",
        "                + template3,\n",
        "              input_variables = [\"topic\",\"person\",\"tone\"],\n",
        "              validate_template = True\n",
        "              )\n",
        "prompt = template.format(topic=\"GAI\", person = \"experienced\", tone = \"formal\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOqFfZO7YTd3",
        "outputId": "65f92dff-3653-4c47-9349-3ebb3b5ac69f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define GAI for experienced people Use a formal tone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatPromptTemplate"
      ],
      "metadata": {
        "id": "u34yP8M-PKwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Prompt to chat models\n",
        "*   Input : List of chat messages\n",
        "\n"
      ],
      "metadata": {
        "id": "JBSY4AznPTph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def from_messages(cls, messages: Sequence[MessageLikeRepresentation]) -> ChatPromptTemplate<br>\n",
        "Create a chat prompt template from a variety of message formats.\n",
        "\n",
        "Args:\n",
        "    messages: sequence of message <br>Representations.<br>\n",
        "          A message can be represented using the following formats: <br>(1) BaseMessagePromptTemplate, <br>(2) BaseMessage, <br>(3) 2-tuple of (message type, template); e.g., (\"human\", \"{user_input}\"), <br>(4) 2-tuple of (message class, template), <br>(4) a string which is shorthand for (\"human\", template); e.g., \"{user_input}\"\n",
        "\n",
        "Returns:\n",
        "    a chat prompt template"
      ],
      "metadata": {
        "id": "yos0PquSTAJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2-tuple of (message type, template)"
      ],
      "metadata": {
        "id": "gZwOECWdWJSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Messages as List*"
      ],
      "metadata": {
        "id": "dboafy8EVCmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You are a bot created to act as customer support\"),\n",
        "        (\"human\", \"Hello, I need some help\"),\n",
        "        (\"ai\",\"Hi, How can I help you?\")\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "HYEYmgWqBBQ2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Messages as Tuple"
      ],
      "metadata": {
        "id": "mcD-ROnpVJaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    (\n",
        "        (\"system\",\"You are a bot created to act as customer support\"),\n",
        "        (\"human\", \"Hello, I need some help\"),\n",
        "        (\"ai\",\"Hi, How can I help you?\")\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "o0iEnW2xUzAw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Message as String"
      ],
      "metadata": {
        "id": "y9u_K1V7VYjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    \"You are a cutomer support bot\"\n",
        ")"
      ],
      "metadata": {
        "id": "FoCryAHnVQLz"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Convert to List of Messages"
      ],
      "metadata": {
        "id": "l7vSO36cWlUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "def format_messages(**kwargs: Any) -> List[BaseMessage]<br>\n",
        "Format the chat template into a list of finalized messages.<br>\n",
        "\n",
        "Args:<br>\n",
        "    **kwargs: keyword arguments to use for filling in template variables\n",
        "              in all the template messages in this chat template.\n",
        "\n",
        "Returns:<br>\n",
        "    list of formatted messages"
      ],
      "metadata": {
        "id": "-ROZiiMtUH4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_messages = chat_template.format_messages()\n",
        "print(chat_messages, type(chat_messages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPMlsPCgUb1J",
        "outputId": "ad78983b-095a-4e25-c536-c742ced34d69"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You are a bot created to act as customer support'), HumanMessage(content='Hello, I need some help'), AIMessage(content='Hi, How can I help you?')] <class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt with system and human messages"
      ],
      "metadata": {
        "id": "L1NUp1SqQRoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You write {blog_type} blogs\"),\n",
        "        (\"human\",\"Give some suggestions for latest in {topic}\"),\n",
        "    ]\n",
        ")\n",
        "print(chat_template)\n",
        "\n",
        "prompt = chat_template.format_messages(blog_type=\"technical\",topic = \"GAI\")\n",
        "print(prompt,type(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiTFVxNkPJup",
        "outputId": "da1c0183-3e78-48da-c93d-f4a040126cf2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['blog_type', 'topic'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['blog_type'], template='You write {blog_type} blogs')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='Give some suggestions for latest in {topic}'))]\n",
            "[SystemMessage(content='You write technical blogs'), HumanMessage(content='Give some suggestions for latest in GAI')] <class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Formatted Prompt:\")\n",
        "print(type(prompt),\"\\n\",prompt)\n",
        "for index, value in enumerate(prompt):\n",
        "    print(f\"Index: {index}, Value: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IuzQFS1BJOA",
        "outputId": "f348f597-840e-4cd9-d7b3-6baee3a3b649"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatted Prompt:\n",
            "<class 'list'> \n",
            " [SystemMessage(content='You write technical blogs'), HumanMessage(content='Give some suggestions for latest in GAI')]\n",
            "Index: 0, Value: content='You write technical blogs'\n",
            "Index: 1, Value: content='Give some suggestions for latest in GAI'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BaseMessagePromptTemplate"
      ],
      "metadata": {
        "id": "BRZcix5_GkHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from_template creates class from string template"
      ],
      "metadata": {
        "id": "QCeUYksQc6H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain.schema import SystemMessage\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "      SystemMessagePromptTemplate.from_template(\"You write technical blogs\"),\n",
        "      HumanMessagePromptTemplate.from_template(\"Give some suggestions for latest in {topic}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "5pPESyZjZ2m6"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BaseMessage"
      ],
      "metadata": {
        "id": "HcKANVpfdaiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.schema import SystemMessage\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "      SystemMessage(content = (\"You write technical blogs\")), # String content of message\n",
        "      HumanMessagePromptTemplate.from_template(\"Give some suggestions for latest in {topic}\")\n",
        "    ]\n",
        ")\n",
        "prompt = chat_template.format_messages(topic=\"GAI\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF1xHRYXCqQK",
        "outputId": "f28d08f3-1549-46f5-db2d-e7321ab507ae"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You write technical blogs'), HumanMessage(content='Give some suggestions for latest in GAI')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "      SystemMessage(content = (\"You write technical blogs\")), # String content of message\n",
        "      HumanMessage(content=\"Give some suggestions for latest in {topic}\")\n",
        "    ]\n",
        ")\n",
        "prompt = chat_template.format_messages(topic=\"GAI\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua3uy1HXdfk3",
        "outputId": "7bfc8ea5-7ba7-4c7a-d87e-b599f9c08e5e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You write technical blogs'), HumanMessage(content='Give some suggestions for latest in {topic}')]\n"
          ]
        }
      ]
    }
  ]
}